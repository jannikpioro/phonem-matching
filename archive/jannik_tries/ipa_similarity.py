#!/usr/bin/env python3
"""
IPA Phonetic Similarity Finder

Findet √§hnlich klingende Wortpaare zwischen Deutsch und Englisch
basierend auf IPA-Transkriptionen.
"""

import re
from collections import defaultdict
from typing import List, Tuple, Dict, Optional
import heapq
from dataclasses import dataclass
from functools import lru_cache
import multiprocessing as mp
from itertools import islice
import json


@dataclass
class Word:
    """Repr√§sentiert ein Wort mit seiner IPA-Transkription."""
    text: str
    ipa: str
    ipa_clean: str  # IPA ohne diakritische Zeichen und Sonderzeichen


def clean_ipa(ipa: str) -> str:
    """
    Bereinigt IPA-String f√ºr Vergleiche.
    Entfernt Betonungszeichen, Silbengrenzen und normalisiert Zeichen.
    """
    # Entferne Slashes und eckige Klammern
    ipa = ipa.strip('/[]')
    
    # Entferne Betonungszeichen und andere diakritische Zeichen
    remove_chars = 'ÀàÀå.ÀëÀê ∞ ∑ ≤À†À§Ã©ÃØÃÉÃàÃäÃö'
    for char in remove_chars:
        ipa = ipa.replace(char, '')
    
    # Normalisiere einige IPA-Zeichen f√ºr besseren Vergleich
    # (z.B. √§hnliche Laute zusammenfassen)
    normalizations = {
        '…´': 'l',  # Dark L zu normalem L
        '…æ': 'r',  # Flap zu R
        '…π': 'r',  # Approximant zu R
        '…ù': '…ê',  # Rhotischer Vokal
        '…ö': '…ô',  # Rhotischer Schwa
        ' Å': 'r',  # Uvular R zu R
        '…ê': 'a',  # Near-open central zu a
        '√¶': 'e',  # Near-open front zu e
        '…õ': 'e',  # Open-mid front zu e
        '…™': 'i',  # Near-close front zu i
        ' ä': 'u',  # Near-close back zu u
        '…î': 'o',  # Open-mid back zu o
        ' å': 'a',  # Open-mid back zu a
        '…ë': 'a',  # Open back zu a
        '≈ã': 'n',  # Velar nasal zu n
        'Œ∏': 's',  # Dental fricative zu s
        '√∞': 'd',  # Voiced dental fricative zu d
        ' É': ' É',  # Behalte sh
        ' í': ' í',  # Behalte zh
        't É': 't É', # Behalte tsch
        'd í': 'd í', # Behalte dsch
        '√ß': ' É',  # Palataler Frikativ zu sch
        'x': 'k',  # Velarer Frikativ zu k
        ' î': '',   # Glottal stop entfernen
    }
    
    for old, new in normalizations.items():
        ipa = ipa.replace(old, new)
    
    return ipa


def parse_ipa_file(filepath: str, max_words: Optional[int] = None, 
                   sample_random: bool = False, min_ipa_length: int = 3,
                   max_ipa_length: int = 15) -> List[Word]:
    """
    Parst eine IPA-Wortliste.
    Format: wort\t/ipa/, /ipa2/
    
    Args:
        filepath: Pfad zur Datei
        max_words: Maximale Anzahl W√∂rter (None = alle)
        sample_random: Zuf√§lliges Sampling statt erste N W√∂rter
        min_ipa_length: Minimale IPA-L√§nge (filtert zu kurze W√∂rter)
        max_ipa_length: Maximale IPA-L√§nge (filtert zu lange/komplexe W√∂rter)
    """
    import random
    
    all_words = []
    
    with open(filepath, 'r', encoding='utf-8') as f:
        for line in f:
            line = line.strip()
            if not line or '\t' not in line:
                continue
            
            parts = line.split('\t')
            if len(parts) < 2:
                continue
            
            text = parts[0].strip()
            ipa_raw = parts[1].strip()
            
            # √úberspringe Eintr√§ge mit Zahlen/Sonderzeichen am Anfang
            if text and text[0].isdigit():
                continue
            
            # Nehme nur die erste Aussprache wenn mehrere vorhanden
            if ',' in ipa_raw:
                ipa_raw = ipa_raw.split(',')[0].strip()
            
            # Entferne Slashes
            ipa = ipa_raw.strip('/')
            ipa_clean = clean_ipa(ipa)
            
            # Filtere nach IPA-L√§nge
            if min_ipa_length <= len(ipa_clean) <= max_ipa_length:
                all_words.append(Word(text=text, ipa=ipa, ipa_clean=ipa_clean))
    
    # Sampling
    if max_words and len(all_words) > max_words:
        if sample_random:
            random.seed(42)  # Reproduzierbar
            words = random.sample(all_words, max_words)
        else:
            words = all_words[:max_words]
    else:
        words = all_words
    
    # Mische die Reihenfolge f√ºr buntere Verarbeitung
    if sample_random:
        random.seed(123)  # Anderer Seed f√ºr Shuffle
        random.shuffle(words)
    
    return words


@lru_cache(maxsize=100000)
def levenshtein_distance(s1: str, s2: str) -> int:
    """
    Berechnet die Levenshtein-Distanz zwischen zwei Strings.
    Cached f√ºr Performance.
    """
    if len(s1) < len(s2):
        return levenshtein_distance(s2, s1)
    
    if len(s2) == 0:
        return len(s1)
    
    previous_row = range(len(s2) + 1)
    
    for i, c1 in enumerate(s1):
        current_row = [i + 1]
        for j, c2 in enumerate(s2):
            insertions = previous_row[j + 1] + 1
            deletions = current_row[j] + 1
            substitutions = previous_row[j] + (c1 != c2)
            current_row.append(min(insertions, deletions, substitutions))
        previous_row = current_row
    
    return previous_row[-1]


def ipa_similarity(ipa1: str, ipa2: str) -> float:
    """
    Berechnet die phonetische √Ñhnlichkeit zwischen zwei IPA-Strings.
    Gibt einen Wert zwischen 0 (v√∂llig unterschiedlich) und 1 (identisch) zur√ºck.
    """
    if not ipa1 or not ipa2:
        return 0.0
    
    distance = levenshtein_distance(ipa1, ipa2)
    max_len = max(len(ipa1), len(ipa2))
    
    # Normalisierte √Ñhnlichkeit
    similarity = 1 - (distance / max_len)
    
    # Bonus f√ºr gleiche L√§nge (klingen oft √§hnlicher)
    length_ratio = min(len(ipa1), len(ipa2)) / max(len(ipa1), len(ipa2))
    
    # Gewichtete Kombination
    return similarity * 0.8 + length_ratio * 0.2


def _save_checkpoint(pairs: List[Tuple], filepath: str, processed: int, total: int):
    """
    Speichert Zwischenergebnisse als JSON.
    """
    # Sortiere nach √Ñhnlichkeit und nehme Top 5000
    sorted_pairs = sorted(pairs, key=lambda x: x[2], reverse=True)[:5000]
    
    results = []
    for source, target, similarity in sorted_pairs:
        results.append({
            'similarity': round(similarity, 4),
            'source': {'word': source.text, 'ipa': source.ipa},
            'target': {'word': target.text, 'ipa': target.ipa}
        })
    
    checkpoint_data = {
        'status': 'in_progress',
        'processed': processed,
        'total': total,
        'progress_percent': round(100 * processed / total, 1),
        'pairs_found': len(pairs),
        'top_pairs': results
    }
    
    with open(filepath, 'w', encoding='utf-8') as f:
        json.dump(checkpoint_data, f, ensure_ascii=False, indent=2)
    
    print(f"  üíæ Checkpoint gespeichert: {processed}/{total} ({len(pairs)} Paare)")


def create_length_index(words: List[Word]) -> Dict[int, List[Word]]:
    """
    Erstellt einen Index nach IPA-L√§nge f√ºr schnellere Suche.
    """
    index = defaultdict(list)
    for word in words:
        length = len(word.ipa_clean)
        index[length].append(word)
    return dict(index)


def create_prefix_index(words: List[Word], prefix_length: int = 2) -> Dict[str, List[Word]]:
    """
    Erstellt einen Prefix-Index f√ºr schnellere Suche.
    """
    index = defaultdict(list)
    for word in words:
        if len(word.ipa_clean) >= prefix_length:
            prefix = word.ipa_clean[:prefix_length]
            index[prefix].append(word)
    return dict(index)


def find_similar_words(
    source_word: Word,
    target_words: List[Word],
    length_index: Dict[int, List[Word]],
    min_similarity: float = 0.5,
    top_n: int = 5,
    length_tolerance: int = 3
) -> List[Tuple[Word, float]]:
    """
    Findet die √§hnlichsten W√∂rter f√ºr ein gegebenes Quellwort.
    Verwendet Length-Index f√ºr Effizienz.
    """
    source_len = len(source_word.ipa_clean)
    candidates = []
    
    # Nur W√∂rter mit √§hnlicher L√§nge betrachten
    for length in range(max(2, source_len - length_tolerance), 
                        source_len + length_tolerance + 1):
        if length in length_index:
            candidates.extend(length_index[length])
    
    # Berechne √Ñhnlichkeit f√ºr alle Kandidaten
    results = []
    for target_word in candidates:
        similarity = ipa_similarity(source_word.ipa_clean, target_word.ipa_clean)
        if similarity >= min_similarity:
            results.append((target_word, similarity))
    
    # Sortiere nach √Ñhnlichkeit und gib Top-N zur√ºck
    results.sort(key=lambda x: x[1], reverse=True)
    return results[:top_n]


def find_all_similar_pairs(
    source_words: List[Word],
    target_words: List[Word],
    min_similarity: float = 0.7,
    top_n_per_word: int = 3,
    total_top_n: int = 1000,
    progress_interval: int = 1000,
    checkpoint_interval: int = 10000,
    checkpoint_file: str = "checkpoint_pairs.json"
) -> List[Tuple[Word, Word, float]]:
    """
    Findet alle √§hnlichen Wortpaare zwischen zwei Sprachen.
    Speichert Zwischenergebnisse alle checkpoint_interval W√∂rter.
    """
    print(f"Erstelle Index f√ºr {len(target_words)} Zielw√∂rter...")
    length_index = create_length_index(target_words)
    
    all_pairs = []
    
    print(f"Vergleiche {len(source_words)} Quellw√∂rter...")
    print(f"Zwischenspeicherung alle {checkpoint_interval} W√∂rter in: {checkpoint_file}")
    
    for i, source_word in enumerate(source_words):
        if (i + 1) % progress_interval == 0:
            print(f"  Fortschritt: {i + 1}/{len(source_words)} ({100*(i+1)/len(source_words):.1f}%) - {len(all_pairs)} Paare gefunden")
        
        # Zwischenspeicherung
        if (i + 1) % checkpoint_interval == 0:
            _save_checkpoint(all_pairs, checkpoint_file, i + 1, len(source_words))
        
        similar = find_similar_words(
            source_word, 
            target_words, 
            length_index,
            min_similarity=min_similarity,
            top_n=top_n_per_word
        )
        
        for target_word, similarity in similar:
            all_pairs.append((source_word, target_word, similarity))
    
    # Sortiere alle Paare nach √Ñhnlichkeit
    print(f"Sortiere {len(all_pairs)} Paare...")
    all_pairs.sort(key=lambda x: x[2], reverse=True)
    
    # Entferne Duplikate (gleiches Wortpaar mit verschiedener Reihenfolge)
    seen = set()
    unique_pairs = []
    for source, target, sim in all_pairs:
        key = (source.text.lower(), target.text.lower())
        if key not in seen:
            seen.add(key)
            unique_pairs.append((source, target, sim))
    
    return unique_pairs[:total_top_n]


def format_results(pairs: List[Tuple[Word, Word, float]], source_lang: str, target_lang: str) -> str:
    """
    Formatiert die Ergebnisse als lesbare Tabelle.
    """
    lines = []
    lines.append(f"\n{'='*80}")
    lines.append(f"TOP {len(pairs)} √ÑHNLICH KLINGENDE WORTPAARE")
    lines.append(f"{source_lang} ‚Üí {target_lang}")
    lines.append(f"{'='*80}\n")
    lines.append(f"{'Rang':<6} {'√Ñhnl.':<8} {source_lang + ' Wort':<25} {target_lang + ' Wort':<25} {'IPA Vergleich'}")
    lines.append(f"{'-'*6} {'-'*8} {'-'*25} {'-'*25} {'-'*40}")
    
    for i, (source, target, similarity) in enumerate(pairs, 1):
        ipa_comparison = f"/{source.ipa}/ ‚Üî /{target.ipa}/"
        lines.append(f"{i:<6} {similarity:>6.1%}   {source.text:<25} {target.text:<25} {ipa_comparison}")
    
    return '\n'.join(lines)


def save_results_json(pairs: List[Tuple[Word, Word, float]], filepath: str):
    """
    Speichert Ergebnisse als JSON.
    """
    results = []
    for source, target, similarity in pairs:
        results.append({
            'rank': len(results) + 1,
            'similarity': round(similarity, 4),
            'source': {
                'word': source.text,
                'ipa': source.ipa,
                'ipa_clean': source.ipa_clean
            },
            'target': {
                'word': target.text,
                'ipa': target.ipa,
                'ipa_clean': target.ipa_clean
            }
        })
    
    with open(filepath, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    
    print(f"Ergebnisse gespeichert in: {filepath}")


def main():
    """
    Hauptfunktion: L√§dt Daten, findet √§hnliche Wortpaare, speichert Ergebnisse.
    """
    import argparse
    
    parser = argparse.ArgumentParser(description='Finde √§hnlich klingende Wortpaare zwischen Sprachen')
    parser.add_argument('--german', '-g', default='data/de (1).txt', help='Pfad zur deutschen Wortliste')
    parser.add_argument('--english', '-e', default='data/en_US.txt', help='Pfad zur englischen Wortliste')
    parser.add_argument('--max-words', '-m', type=int, default=10000, help='Maximale Anzahl W√∂rter pro Sprache (f√ºr schnellere Tests)')
    parser.add_argument('--min-similarity', '-s', type=float, default=0.7, help='Minimale √Ñhnlichkeit (0-1)')
    parser.add_argument('--top-n', '-n', type=int, default=100, help='Anzahl der Top-Ergebnisse')
    parser.add_argument('--output', '-o', default='similar_words.json', help='Ausgabedatei (JSON)')
    parser.add_argument('--random', '-r', action='store_true', help='Zuf√§lliges Sampling statt erste N W√∂rter')
    parser.add_argument('--min-length', type=int, default=3, help='Minimale IPA-L√§nge')
    parser.add_argument('--max-length', type=int, default=12, help='Maximale IPA-L√§nge')
    
    args = parser.parse_args()
    
    print(f"\n{'='*60}")
    print("IPA PHONETISCHE √ÑHNLICHKEITS-ANALYSE")
    print(f"{'='*60}\n")
    
    # Lade Wortlisten
    print(f"Lade deutsche W√∂rter aus: {args.german}")
    german_words = parse_ipa_file(
        args.german, 
        max_words=args.max_words,
        sample_random=args.random,
        min_ipa_length=args.min_length,
        max_ipa_length=args.max_length
    )
    print(f"  ‚Üí {len(german_words)} W√∂rter geladen")
    
    print(f"Lade englische W√∂rter aus: {args.english}")
    english_words = parse_ipa_file(
        args.english, 
        max_words=args.max_words,
        sample_random=args.random,
        min_ipa_length=args.min_length,
        max_ipa_length=args.max_length
    )
    print(f"  ‚Üí {len(english_words)} W√∂rter geladen\n")
    
    # Finde √§hnliche Paare (Deutsch ‚Üí Englisch)
    print("Suche √§hnlich klingende Wortpaare (Deutsch ‚Üí Englisch)...")
    pairs = find_all_similar_pairs(
        german_words,
        english_words,
        min_similarity=args.min_similarity,
        top_n_per_word=3,
        total_top_n=args.top_n
    )
    
    # Zeige Ergebnisse
    print(format_results(pairs, "Deutsch", "Englisch"))
    
    # Speichere als JSON
    save_results_json(pairs, args.output)
    
    print(f"\n‚úì Analyse abgeschlossen!")
    print(f"  {len(pairs)} √§hnliche Wortpaare gefunden")


if __name__ == '__main__':
    main()
